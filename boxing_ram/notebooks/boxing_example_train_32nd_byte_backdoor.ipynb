{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "os.chdir('../')\n",
    "code_path = os.getcwd()\n",
    "sys.path.append(code_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level imports and logging config\n",
    "\n",
    "import logging.config\n",
    "import os, sys\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "\n",
    "from trojai_rl.datagen.environment_factory import EnvironmentFactory\n",
    "from trojai_rl.datagen.envs.wrapped_boxing_with_trigger import WrappedBoxingConfig, WrappedBoxing\n",
    "from trojai_rl.modelgen.architectures.atari_architectures import FC512Model\n",
    "from trojai_rl.modelgen.config import RunnerConfig, TestConfig\n",
    "from trojai_rl.modelgen.runner import Runner\n",
    "from trojai_rl.modelgen.torch_ac_optimizer import TorchACOptimizer, TorchACOptConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.config.dictConfig({\n",
    "        'version': 1,\n",
    "        'formatters': {\n",
    "            'basic': {\n",
    "                'format': '%(message)s',\n",
    "            },\n",
    "            'detailed': {\n",
    "                'format': '[%(asctime)s] %(levelname)s in %(module)s: %(message)s',\n",
    "            },\n",
    "        },\n",
    "        'handlers': {\n",
    "            'console': {\n",
    "                'class': 'logging.StreamHandler',\n",
    "                'formatter': 'basic',\n",
    "                'level': 'WARNING',\n",
    "            }\n",
    "        },\n",
    "        'loggers': {\n",
    "            'trojai_rl': {\n",
    "                'handlers': ['console'],\n",
    "            },\n",
    "        },\n",
    "        'root': {\n",
    "            'level': 'INFO',\n",
    "        },\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"  # sometimes required for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Boxing Wrapper and Environment Factory\n",
    "\n",
    "class BoxingRAMObsWrapper(gym.Wrapper):\n",
    "    \"\"\"Observation wrapper for Boxing with RAM observation space. Modifies the observations by:\n",
    "        - masking RAM vector to only include player location, ball location, score, and number of blocks hit.\n",
    "        - stacking 'steps' number of steps into one observation.\n",
    "        - modifying reward signal to be -1, 0, or 1.\n",
    "        - normalize observation vector to float values between 0 and 1.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, boxing_env, steps=4):\n",
    "        super().__init__(boxing_env)\n",
    "        self.steps = steps\n",
    "        self._frames = deque(maxlen=self.steps)\n",
    "\n",
    "        self.boxing_mapping = [17, 18, 19, 32, 33, 34, 35, 100]\n",
    "        # clock, player_score, enemy_score, player_x, enemy_x, player_y, enemy_y\n",
    "        # https://github.com/mila-iqia/atari-representation-learning/blob/master/atariari/benchmark/ram_annotations.py\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, shape=(8 * self.steps,))\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset()\n",
    "        obs = self._process_state(obs)\n",
    "        for _ in range(self.steps):\n",
    "            self._frames.append(obs)\n",
    "        return np.concatenate(self._frames)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self._frames.append(self._process_state(obs))\n",
    "        reward = np.sign(reward)\n",
    "        return np.concatenate(self._frames), reward, done, info\n",
    "\n",
    "    def _process_state(self, obs):\n",
    "        return obs[self.boxing_mapping].astype(np.float32) / 255.0\n",
    "    \n",
    "class RAMEnvFactory(EnvironmentFactory):\n",
    "    def new_environment(self, *args, **kwargs):\n",
    "        return BoxingRAMObsWrapper(WrappedBoxing(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TorchACOptConfig custom measurement and early stopping handles\n",
    "\n",
    "# TorchACOptConfig functions; see modelgen/torch_ac_optimizer.py\n",
    "def eval_stats(**kwargs):\n",
    "    rewards = kwargs['rewards']\n",
    "    steps = kwargs['steps']\n",
    "    test_cfg = kwargs['test_cfg']\n",
    "    env = kwargs['env']\n",
    "\n",
    "    # note that numpy types are not json serializable\n",
    "    eval_results = {}\n",
    "    reward_sums = [float(np.sum(run)) for run in rewards]\n",
    "    eval_results['reward_sums'] = reward_sums\n",
    "    eval_results['reward_avg'] = float(np.mean(reward_sums))\n",
    "    eval_results['steps'] = steps\n",
    "    eval_results['steps_avg'] = float(np.mean(steps))\n",
    "    eval_results['poison'] = env.poison\n",
    "    eval_results['poison_behavior'] = env.poison_behavior\n",
    "    eval_results['argmax_action'] = test_cfg.get_argmax_action()\n",
    "    return eval_results\n",
    "\n",
    "\n",
    "def aggregate_results(results_list):\n",
    "    results = {'clean_reward_avgs': [], 'poison_reward_avgs': [], 'clean_step_avgs': [], 'poison_step_avgs': []}\n",
    "    for res in results_list:\n",
    "        if res['poison']:\n",
    "            results['poison_reward_avgs'].append(res['reward_avg'])\n",
    "            results['poison_step_avgs'].append(res['steps_avg'])\n",
    "        else:\n",
    "            results['clean_reward_avgs'].append(res['reward_avg'])\n",
    "            results['clean_step_avgs'].append(res['steps_avg'])\n",
    "    agg_results = {\n",
    "        \"clean_rewards_avg\": float(np.mean(results['clean_reward_avgs'])),\n",
    "        \"clean_step_avg\": float(np.mean(results['clean_step_avgs'])),\n",
    "        \"poison_rewards_avg\": float(np.mean(results['poison_reward_avgs'])),\n",
    "        \"poison_step_avg\": float(np.mean(results['poison_step_avgs'])),\n",
    "        \"detailed_results\": results_list\n",
    "    }\n",
    "    # Note: This can be a good place to print intermediate results to console, e.g.:\n",
    "    #       logger.debug(\"\")\n",
    "    #       logger.debug(\"clean rewards avg:\", agg_results['clean_rewards_avg'])\n",
    "    #       logger.debug(\"poison rewards avg:\", agg_results['poison_rewards_avg'])\n",
    "    return agg_results\n",
    "\n",
    "\n",
    "def early_stop(**kwargs):\n",
    "    agg_results = kwargs['aggregated_test_results']\n",
    "    logs1 = kwargs['logs1']\n",
    "    logs2 = kwargs['logs2']\n",
    "    opt_cfg = kwargs['optimizer_cfg']\n",
    "\n",
    "    # stop at a level greater than human performance, which is 4.3:\n",
    "    # From Nature paper (https://www.nature.com/articles/nature14236):\n",
    "    #   random: 0.1, LL: 44, DQN: 71.8 (+-8.4), human: 4.3\n",
    "    # From PPO paper (https://arxiv.org/pdf/1707.06347.pdf)\n",
    "    #   A2C: 17.7, ACER: 98.6, PPO: 94.6\n",
    "\n",
    "    # Note that human-level performance is 12.1 in (https://arxiv.org/pdf/1511.06581.pdf)\n",
    "\n",
    "    # Max score is 100\n",
    "\n",
    "    # Note: We normalize rewards to be -1, 0, and 1, even though it is possible to get values like -2 and 2\n",
    "    #  occasionally due to rewards being aggregated over k frames, k uniformly sampled from {2, 3, 4};\n",
    "    #  https://gym.openai.com/envs/Boxing-v0/. Consequently, scores here may be slightly off, but shouldn't be far off\n",
    "    #  since this isn't a common occurrence. Setting the stopping criteria to 35 should produce agents that surpass\n",
    "    #  human performance despite reward normalization for clean environments. For triggered performance, setting all\n",
    "    #  actions to 0 produces a score of -52 on a clean environment.\n",
    "    if any([env_cfg.poison for env_cfg in opt_cfg.train_cfgs]):\n",
    "        if agg_results['clean_rewards_avg'] > 50 and agg_results['poison_rewards_avg'] < -25:\n",
    "            return True\n",
    "    else:\n",
    "        if agg_results['clean_rewards_avg'] > 50:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intermediate_testing_data(pretrained=True):\n",
    "    \"\"\"\n",
    "    Plot intermittent testing information using saved JSON file created after training\n",
    "    :param pretrained: (bool) Use data from the pretrained model included in the repository; assumes the data has\n",
    "        not been moved\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    if pretrained:\n",
    "        with open('pretrained_boxing/FC512Model.pt.train.stats.json') as f:  \n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        with open('stats/BoxingFC512Model.pt.train.stats.json') as f:  \n",
    "            data = json.load(f)\n",
    "\n",
    "    clean_avgs = []\n",
    "    poison_avgs = []\n",
    "    for v in data['intermediate_test_results']:\n",
    "        clean_avgs.append(v['clean_rewards_avg'])\n",
    "        poison_avgs.append(v['poison_rewards_avg'])\n",
    "\n",
    "    plt.plot(range(len(clean_avgs)), clean_avgs, label='clean')\n",
    "    plt.plot(range(len(poison_avgs)), poison_avgs, label='triggered')\n",
    "    plt.title(\"Boxing-ram-v0 Intermediate Test Performance\")\n",
    "    plt.xlabel(\"Test number (~100,000 frames or ~78 optim steps)\")\n",
    "    plt.ylabel(\"Avg score over 20 games\")\n",
    "    plt.show("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train an agent\n",
    "# train_boxing_model()\n",
    "\n",
    "# def train_boxing_model():\n",
    "# training parameters\n",
    "\n",
    "poison = 'poison_100th_byte'\n",
    "poison_behavior = 'negate_reward'\n",
    "test_poison_behavior = 'no_change'  # turns off reward modifications for more intuitive performance measurments\n",
    "num_clean_train_envs = 8\n",
    "num_triggered_train_envs = 2\n",
    "max_frames = int(30e6)  # early stopping should stop in less than 25 million frames for this example\n",
    "num_frames_per_proc = 128\n",
    "num_epochs = 3\n",
    "test_freq_frames = 100000  # do intermediate testing every this many frames trained on\n",
    "int_num_clean_test = 30  # number of clean environment tests to run during intermittent testing\n",
    "int_num_triggered_test = 30  # number of triggered environment tests to run during intermittent testing\n",
    "num_clean_test = 30  # number of clean environment tests to run at end of training\n",
    "num_triggered_test = 30  # number of triggered environment tests to run at end of training\n",
    "test_max_steps = 2450  # just more than enough to run the full game if neither agent gets a KO\n",
    "\n",
    "\n",
    "episode_batch_size = 100  # for every episode_batch_size episodes, run clean_episode_batch_size episodes.\n",
    "clean_episode_batch_size = 90\n",
    "\n",
    "# Note: Boxing runs can take a long time, and can significantly slow training with intermittent testing. \n",
    "#   Altering test_freq_frames, int_num_clean_test, and int_num_triggered_test will affect this most:\n",
    "#      Set test_freq_frames to np.inf to not do intermittent testing\n",
    "#      Increase test_freq_frames to test less frequently, but will examine stopping criteria less often also\n",
    "#      Decrease int_num_clean_test and/or int_num_triggered_test to run fewer tests, but may make\n",
    "#         averages less reliable; or set them to 0 to run no tests, which also affects early stopping\n",
    "\n",
    "save_loc = os.path.abspath('./')\n",
    "learning_rate = 0.0001\n",
    "value_loss_coef=1.0\n",
    "clip_eps=0.1\n",
    "model_save_name = \"BoxingFC512Model.pt\"\n",
    "\n",
    "device = 'cpu'\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# set up training configs\n",
    "\n",
    "train_env_factory = RAMEnvFactory()\n",
    "test_env_factory = RAMEnvFactory()\n",
    "\n",
    "clean_train_args = dict()\n",
    "triggered_train_args = dict(poison=poison, poison_behavior=poison_behavior, episode_batch_size=episode_batch_size, clean_episode_batch_size=clean_episode_batch_size)\n",
    "poison_test_args = dict(poison=poison, poison_behavior=test_poison_behavior,\\\n",
    "                       episode_batch_size=episode_batch_size, clean_episode_batch_size=clean_episode_batch_size)\n",
    "\n",
    "train_env_cfgs = [WrappedBoxingConfig(**clean_train_args) for _ in range(num_clean_train_envs)] + \\\n",
    "                 [WrappedBoxingConfig(**triggered_train_args) for _ in range(num_triggered_train_envs)]\n",
    "\n",
    "intermediate_test_cfgs = [TestConfig(WrappedBoxingConfig(**clean_train_args), count=int_num_clean_test),\n",
    "                          TestConfig(WrappedBoxingConfig(**poison_test_args), count=int_num_triggered_test)]\n",
    "\n",
    "test_cfgs = [TestConfig(WrappedBoxingConfig(**clean_train_args), count=num_clean_test),\n",
    "             TestConfig(WrappedBoxingConfig(**poison_test_args), count=num_triggered_test)]\n",
    "\n",
    "#--- clean training environment initialized\n",
    "env = BoxingRAMObsWrapper(WrappedBoxing(WrappedBoxingConfig(**triggered_train_args)))\n",
    "model = FC512Model(env.observation_space, env.action_space)\n",
    "model.to(device)\n",
    "\n",
    "# set up optimizer\n",
    "optimizer_cfg = TorchACOptConfig(train_env_cfgs=train_env_cfgs,\n",
    "                                 test_cfgs=test_cfgs,\n",
    "                                 algorithm='ppo',\n",
    "                                 num_frames=max_frames,\n",
    "                                 num_frames_per_proc=num_frames_per_proc,\n",
    "                                 epochs=num_epochs,\n",
    "                                 test_freq_frames=test_freq_frames,\n",
    "                                 test_max_steps=test_max_steps,\n",
    "                                 learning_rate=learning_rate,\n",
    "                                 value_loss_coef=value_loss_coef,\n",
    "                                 clip_eps=clip_eps,\n",
    "                                 device=device,\n",
    "                                 intermediate_test_cfgs=intermediate_test_cfgs,\n",
    "                                 eval_stats=eval_stats,\n",
    "                                 aggregate_test_results=aggregate_results,\n",
    "                                 early_stop=early_stop,\n",
    "                                 preprocess_obss=model.preprocess_obss)\n",
    "optimizer = TorchACOptimizer(optimizer_cfg)\n",
    "\n",
    "# turn arguments into a dictionary that we can save as run information\n",
    "save_info = dict(poison=poison, \n",
    "                 poison_behavior=poison_behavior, \n",
    "                 test_poison_behavior=test_poison_behavior, \n",
    "                 num_clean_train_envs=num_clean_train_envs,\n",
    "                 num_triggered_train_envs=num_triggered_train_envs,\n",
    "                 max_frames=max_frames,\n",
    "                 num_frames_per_proc=num_frames_per_proc,\n",
    "                 num_epochs=num_epochs, \n",
    "                 test_freq_frames=test_freq_frames, \n",
    "                 int_num_clean_test=int_num_clean_test,\n",
    "                 int_num_triggered_test=int_num_triggered_test,\n",
    "                 num_clean_test=num_clean_test,\n",
    "                 num_triggered_test=num_triggered_test,\n",
    "                 test_max_steps=test_max_steps,\n",
    "                 save_loc=save_loc\n",
    "                )\n",
    "\n",
    "# set up runner and create model\n",
    "runner_cfg = RunnerConfig(train_env_factory, test_env_factory, model, optimizer,\n",
    "                          model_save_dir=os.path.join(save_loc, 'models/'),\n",
    "                          stats_save_dir=os.path.join(save_loc, 'stats/'),\n",
    "                          filename=model_save_name,\n",
    "                          save_info=save_info)\n",
    "runner = Runner(runner_cfg)\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intermediate_testing_data(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stat = runner.runner_cfg.optimizer.test(model, runner_cfg.test_env_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_stat.agg_results['detailed_results']\n",
    "reward_sums1 = results[0]['reward_sums']\n",
    "reward_sums2 = results[1]['reward_sums']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(len(reward_sums1)), reward_sums1, label='clean')\n",
    "plt.plot(range(len(reward_sums2)), reward_sums2, label='poison_100th_byte')\n",
    "plt.legend()\n",
    "plt.xlabel('Test episode #')\n",
    "plt.ylabel('Episodic return')\n",
    "plt.title(\"Test Return Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_cfgs = [TestConfig(WrappedBoxingConfig(**clean_train_args), count=100),\n",
    "             TestConfig(WrappedBoxingConfig(**poison_test_args), count=100)]\n",
    "\n",
    "runner.runner_cfg.optimizer.config.test_cfgs = new_test_cfgs\n",
    "new_test_stat = runner.runner_cfg.optimizer.test(model, runner_cfg.test_env_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = new_test_stat.agg_results['detailed_results']\n",
    "reward_sums1 = results[0]['reward_sums']\n",
    "reward_sums2 = results[1]['reward_sums']\n",
    "\n",
    "plt.plot(range(len(reward_sums1)), reward_sums1, label='clean')\n",
    "plt.plot(range(len(reward_sums2)), reward_sums2, label='poison_100th_byte')\n",
    "plt.legend()\n",
    "plt.xlabel('Test episode #')\n",
    "plt.ylabel('Episodic return')\n",
    "plt.title(\"Test Return Plot\")\n",
    "plt.savefig('images/test_return_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Return for clean episodes - mean : {0:2.2f} std : {1:2.2f}'.format(np.mean(reward_sums1),np.std(reward_sums1)))\n",
    "print('Return for poisoned episodes - mean : {0:2.2f} std : {1:2.2f}'.format(np.mean(reward_sums2),np.std(reward_sums2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_factory = runner.runner_cfg.test_env_factory\n",
    "my_optimizer = runner.runner_cfg.optimizer\n",
    "\n",
    "new_test_cfgs = [TestConfig(WrappedBoxingConfig(**clean_train_args), count=1),\n",
    "             TestConfig(WrappedBoxingConfig(**poison_test_args), count=1)]\n",
    "\n",
    "runner.runner_cfg.optimizer.config.test_cfgs = new_test_cfgs\n",
    "test_cfgs = my_optimizer.config.test_cfgs\n",
    "\n",
    "angle=90\n",
    "for k, test_cfg in enumerate(test_cfgs):\n",
    "    env = env_factory.new_environment(test_cfg.env_cfg)\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    actions = []\n",
    "    info_dicts = []\n",
    "    for i in range(test_cfg.count):\n",
    "        print(i)\n",
    "        run_rewards, run_steps, run_actions, run_info_dicts = my_optimizer.test_env(env, model, my_optimizer.config.test_max_steps,\n",
    "                                                                            argmax_action=test_cfg.argmax_action)\n",
    "        \n",
    "        action_to_name = ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT', 'UPFIRE', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n",
    "        run_actions_int = [action_to_name[int(action)] for action in run_actions]\n",
    "        \n",
    "        \n",
    "        plt.hist(run_actions_int, bins=np.arange(19))\n",
    "        plt.xticks(np.arange(0, 19, step=1))          \n",
    "        plt.xlabel('action #')\n",
    "        plt.ylabel('count')\n",
    "        plt.xticks(rotation = angle)\n",
    "        if(k==0):\n",
    "            sr = pd.Series(run_actions_int).value_counts()\n",
    "            sr.to_csv('images/clean_count.csv')\n",
    "            plt.title('Histogram of actions in clean episode.')\n",
    "            plt.savefig('images/actions_clean.png')\n",
    "        else:\n",
    "            pd.Series(run_actions_int).value_counts().to_csv('images/poison_count.csv')\n",
    "            plt.title('Histogram of actions in poisoned episode.')\n",
    "            plt.savefig('images/actions_poison.png')\n",
    "            \n",
    "        plt.show()\n",
    "        \n",
    "        plt.hist(run_rewards, bins=np.arange(-5,5))\n",
    "        plt.xticks(np.arange(-5, 5, step=1))            \n",
    "        plt.xlabel('reward')\n",
    "        plt.ylabel('count')\n",
    "        plt.ylim((0,100))\n",
    "        if(k==0):\n",
    "            plt.title('Histogram of rewards in clean episode.')\n",
    "            plt.savefig('images/rewards_clean_zoom.png')\n",
    "        else:\n",
    "            plt.title('Histogram of rewards in poisoned episode.')\n",
    "            plt.savefig('images/rewards_poison_zoom.png')\n",
    "            \n",
    "        plt.show()\n",
    "        \n",
    "        plt.hist(run_rewards, bins=np.arange(-5,5))\n",
    "        plt.xticks(np.arange(-5, 5, step=1))\n",
    "        plt.xlabel('action #')\n",
    "        plt.ylabel('count')\n",
    "        if(k==0):\n",
    "            plt.title('Histogram of rewards in clean episode.')\n",
    "            plt.savefig('images/rewards_clean.png')\n",
    "        else:\n",
    "            plt.title('Histogram of rewards in poisoned episode.')\n",
    "            plt.savefig('images/rewards_poison.png')\n",
    "            \n",
    "        plt.show()\n",
    "        \n",
    "        rewards.append(run_rewards)\n",
    "        steps.append(run_steps)\n",
    "        actions.append(run_actions)\n",
    "        info_dicts.append(run_info_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions1 = [int(action) for action in actions[0][:2189]]\n",
    "actions2 = [int(action) for action in actions[0][2189:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(run_actions_int, bins=np.arange(18))\n",
    "plt.xticks(np.arange(0, 18, step=1))\n",
    "plt.ylim((0,100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_names = ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT', 'UPFIRE', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "pd.Series(run_actions_int).value_counts().to_csv('images/poison_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = True\n",
    "\n",
    "if pretrained:\n",
    "    with open('notebooks/pretrained_boxing/FC512Model.pt.train.stats.json') as f:  \n",
    "        data = json.load(f)\n",
    "else:\n",
    "    with open('notebooks/stats/BoxingFC512Model.pt.train.stats.json') as f:  \n",
    "        data = json.load(f)\n",
    "\n",
    "clean_avgs = []\n",
    "poison_avgs = []\n",
    "for v in data['intermediate_test_results']:\n",
    "    clean_avgs.append(v['clean_rewards_avg'])\n",
    "    poison_avgs.append(v['poison_rewards_avg'])\n",
    "\n",
    "plt.plot(range(len(clean_avgs)), clean_avgs, label='clean')\n",
    "plt.plot(range(len(poison_avgs)), poison_avgs, label='triggered')\n",
    "plt.title(\"Boxing-ram-v0 Intermediate Test Performance\")\n",
    "plt.xlabel(\"Test number (~100,000 frames or ~78 optim steps)\")\n",
    "plt.ylabel(\"Avg score over 20 games\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_factory = runner.runner_cfg.test_env_factory\n",
    "my_optimizer = runner.runner_cfg.optimizer\n",
    "\n",
    "new_test_cfgs = [TestConfig(WrappedBoxingConfig(**clean_train_args), count=1),\n",
    "             TestConfig(WrappedBoxingConfig(**poison_test_args), count=1)]\n",
    "\n",
    "runner.runner_cfg.optimizer.config.test_cfgs = new_test_cfgs\n",
    "test_cfgs = my_optimizer.config.test_cfgs\n",
    "\n",
    "angle=90\n",
    "for k, test_cfg in enumerate(test_cfgs):\n",
    "    env = env_factory.new_environment(test_cfg.env_cfg)\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    actions = []\n",
    "    info_dicts = []\n",
    "    for i in range(test_cfg.count):\n",
    "        print(i)\n",
    "        run_rewards, run_steps, run_actions, run_info_dicts, screens = my_optimizer.test_env(env, model, my_optimizer.config.test_max_steps,\n",
    "                                                                            argmax_action=test_cfg.argmax_action)\n",
    "        \n",
    "        action_to_name = ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT', 'UPFIRE', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n",
    "        run_actions_int = [action_to_name[int(action)] for action in run_actions]\n",
    "        \n",
    "        \n",
    "        plt.hist(run_actions_int, bins=np.arange(19))\n",
    "        plt.xticks(np.arange(0, 19, step=1))          \n",
    "        plt.xlabel('action #')\n",
    "        plt.ylabel('count')\n",
    "        plt.xticks(rotation = angle)\n",
    "        if(k==0):\n",
    "            sr = pd.Series(run_actions_int).value_counts()\n",
    "            sr.to_csv('images/clean_count.csv')\n",
    "            plt.title('Histogram of actions in clean episode.')\n",
    "            plt.savefig('images/actions_clean.png')\n",
    "        else:\n",
    "            pd.Series(run_actions_int).value_counts().to_csv('images/poison_count.csv')\n",
    "            plt.title('Histogram of actions in poisoned episode.')\n",
    "            plt.savefig('images/actions_poison.png')\n",
    "            \n",
    "        plt.show()\n",
    "        \n",
    "        plt.hist(run_rewards, bins=np.arange(-5,5))\n",
    "        plt.xticks(np.arange(-5, 5, step=1))            \n",
    "        plt.xlabel('reward')\n",
    "        plt.ylabel('count')\n",
    "        plt.ylim((0,100))\n",
    "        if(k==0):\n",
    "            plt.title('Histogram of rewards in clean episode.')\n",
    "            plt.savefig('images/rewards_clean_zoom.png')\n",
    "        else:\n",
    "            plt.title('Histogram of rewards in poisoned episode.')\n",
    "            plt.savefig('images/rewards_poison_zoom.png')\n",
    "            \n",
    "        plt.show()\n",
    "        \n",
    "        plt.hist(run_rewards, bins=np.arange(-5,5))\n",
    "        plt.xticks(np.arange(-5, 5, step=1))\n",
    "        plt.xlabel('action #')\n",
    "        plt.ylabel('count')\n",
    "        if(k==0):\n",
    "            plt.title('Histogram of rewards in clean episode.')\n",
    "            plt.savefig('images/rewards_clean.png')\n",
    "        else:\n",
    "            plt.title('Histogram of rewards in poisoned episode.')\n",
    "            plt.savefig('images/rewards_poison.png')\n",
    "            \n",
    "        plt.show()\n",
    "        \n",
    "        rewards.append(run_rewards)\n",
    "        steps.append(run_steps)\n",
    "        actions.append(run_actions)\n",
    "        info_dicts.append(run_info_dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "troj_rl",
   "language": "python",
   "name": "troj_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
