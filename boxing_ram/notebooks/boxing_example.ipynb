{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level imports and logging config\n",
    "\n",
    "import logging.config\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from trojai_rl.datagen.environment_factory import EnvironmentFactory\n",
    "from trojai_rl.datagen.envs.wrapped_boxing_public import WrappedBoxingConfig, WrappedBoxing\n",
    "from trojai_rl.modelgen.architectures.atari_architectures_public import FC512Model\n",
    "from trojai_rl.modelgen.config import RunnerConfig, TestConfig\n",
    "from trojai_rl.modelgen.runner import Runner\n",
    "from trojai_rl.modelgen.torch_ac_optimizer import TorchACOptimizer, TorchACOptConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.config.dictConfig({\n",
    "        'version': 1,\n",
    "        'formatters': {\n",
    "            'basic': {\n",
    "                'format': '%(message)s',\n",
    "            },\n",
    "            'detailed': {\n",
    "                'format': '[%(asctime)s] %(levelname)s in %(module)s: %(message)s',\n",
    "            },\n",
    "        },\n",
    "        'handlers': {\n",
    "            'console': {\n",
    "                'class': 'logging.StreamHandler',\n",
    "                'formatter': 'basic',\n",
    "                'level': 'WARNING',\n",
    "            }\n",
    "        },\n",
    "        'loggers': {\n",
    "            'trojai_rl': {\n",
    "                'handlers': ['console'],\n",
    "            },\n",
    "        },\n",
    "        'root': {\n",
    "            'level': 'INFO',\n",
    "        },\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"  # sometimes required for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Boxing Wrapper and Environment Factory\n",
    "\n",
    "class BoxingRAMObsWrapper(gym.Wrapper):\n",
    "    \"\"\"Observation wrapper for Boxing with RAM observation space. Modifies the observations by:\n",
    "        - masking RAM vector to only include player location, ball location, score, and number of blocks hit.\n",
    "        - stacking 'steps' number of steps into one observation.\n",
    "        - modifying reward signal to be -1, 0, or 1.\n",
    "        - normalize observation vector to float values between 0 and 1.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, boxing_env, steps=4):\n",
    "        super().__init__(boxing_env)\n",
    "        self.steps = steps\n",
    "        self._frames = deque(maxlen=self.steps)\n",
    "        self.boxing_mapping = [17, 18, 19, 32, 33, 34, 35]\n",
    "        # clock, player_score, enemy_score, player_x, enemy_x, player_y, enemy_y\n",
    "        # https://github.com/mila-iqia/atari-representation-learning/blob/master/atariari/benchmark/ram_annotations.py\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, shape=(7 * self.steps,))\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset()\n",
    "        obs = self._process_state(obs)\n",
    "        for _ in range(self.steps):\n",
    "            self._frames.append(obs)\n",
    "        return np.concatenate(self._frames)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self._frames.append(self._process_state(obs))\n",
    "        reward = np.sign(reward)\n",
    "        return np.concatenate(self._frames), reward, done, info\n",
    "\n",
    "    def _process_state(self, obs):\n",
    "        return obs[self.boxing_mapping].astype(np.float32) / 255.0\n",
    "    \n",
    "class RAMEnvFactory(EnvironmentFactory):\n",
    "    def new_environment(self, *args, **kwargs):\n",
    "        return BoxingRAMObsWrapper(WrappedBoxing(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TorchACOptConfig custom measurement and early stopping handles\n",
    "\n",
    "# TorchACOptConfig functions; see modelgen/torch_ac_optimizer.py\n",
    "def eval_stats(**kwargs):\n",
    "    rewards = kwargs['rewards']\n",
    "    steps = kwargs['steps']\n",
    "    test_cfg = kwargs['test_cfg']\n",
    "    env = kwargs['env']\n",
    "\n",
    "    # note that numpy types are not json serializable\n",
    "    eval_results = {}\n",
    "    reward_sums = [float(np.sum(run)) for run in rewards]\n",
    "    eval_results['reward_sums'] = reward_sums\n",
    "    eval_results['reward_avg'] = float(np.mean(reward_sums))\n",
    "    eval_results['steps'] = steps\n",
    "    eval_results['steps_avg'] = float(np.mean(steps))\n",
    "    eval_results['poison'] = env.poison\n",
    "    eval_results['poison_behavior'] = env.poison_behavior\n",
    "    eval_results['argmax_action'] = test_cfg.get_argmax_action()\n",
    "    return eval_results\n",
    "\n",
    "\n",
    "def aggregate_results(results_list):\n",
    "    results = {'clean_reward_avgs': [], 'poison_reward_avgs': [], 'clean_step_avgs': [], 'poison_step_avgs': []}\n",
    "    for res in results_list:\n",
    "        if res['poison']:\n",
    "            results['poison_reward_avgs'].append(res['reward_avg'])\n",
    "            results['poison_step_avgs'].append(res['steps_avg'])\n",
    "        else:\n",
    "            results['clean_reward_avgs'].append(res['reward_avg'])\n",
    "            results['clean_step_avgs'].append(res['steps_avg'])\n",
    "    agg_results = {\n",
    "        \"clean_rewards_avg\": float(np.mean(results['clean_reward_avgs'])),\n",
    "        \"clean_step_avg\": float(np.mean(results['clean_step_avgs'])),\n",
    "        \"poison_rewards_avg\": float(np.mean(results['poison_reward_avgs'])),\n",
    "        \"poison_step_avg\": float(np.mean(results['poison_step_avgs'])),\n",
    "        \"detailed_results\": results_list\n",
    "    }\n",
    "    # Note: This can be a good place to print intermediate results to console, e.g.:\n",
    "    #       logger.debug(\"\")\n",
    "    #       logger.debug(\"clean rewards avg:\", agg_results['clean_rewards_avg'])\n",
    "    #       logger.debug(\"poison rewards avg:\", agg_results['poison_rewards_avg'])\n",
    "    return agg_results\n",
    "\n",
    "\n",
    "def early_stop(**kwargs):\n",
    "    agg_results = kwargs['aggregated_test_results']\n",
    "    logs1 = kwargs['logs1']\n",
    "    logs2 = kwargs['logs2']\n",
    "    opt_cfg = kwargs['optimizer_cfg']\n",
    "\n",
    "    # stop at a level greater than human performance, which is 4.3:\n",
    "    # From Nature paper (https://www.nature.com/articles/nature14236):\n",
    "    #   random: 0.1, LL: 44, DQN: 71.8 (+-8.4), human: 4.3\n",
    "    # From PPO paper (https://arxiv.org/pdf/1707.06347.pdf)\n",
    "    #   A2C: 17.7, ACER: 98.6, PPO: 94.6\n",
    "\n",
    "    # Note that human-level performance is 12.1 in (https://arxiv.org/pdf/1511.06581.pdf)\n",
    "\n",
    "    # Max score is 100\n",
    "\n",
    "    # Note: We normalize rewards to be -1, 0, and 1, even though it is possible to get values like -2 and 2\n",
    "    #  occasionally due to rewards being aggregated over k frames, k uniformly sampled from {2, 3, 4};\n",
    "    #  https://gym.openai.com/envs/Boxing-v0/. Consequently, scores here may be slightly off, but shouldn't be far off\n",
    "    #  since this isn't a common occurrence. Setting the stopping criteria to 35 should produce agents that surpass\n",
    "    #  human performance despite reward normalization for clean environments. For triggered performance, setting all\n",
    "    #  actions to 0 produces a score of -52 on a clean environment.\n",
    "    if any([env_cfg.poison for env_cfg in opt_cfg.train_cfgs]):\n",
    "        if agg_results['clean_rewards_avg'] > 50 and agg_results['poison_rewards_avg'] < -25:\n",
    "            return True\n",
    "    else:\n",
    "        if agg_results['clean_rewards_avg'] > 50:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_boxing_model():\n",
    "    \n",
    "    # training parameters\n",
    "    \n",
    "    poison = 'add_100'\n",
    "    poison_behavior = 'negate_reward'\n",
    "    test_poison_behavior = 'no_change'  # turns off reward modifications for more intuitive performance measurments\n",
    "    num_clean_train_envs = 8\n",
    "    num_triggered_train_envs = 2\n",
    "    max_frames = int(50e6)  # early stopping should stop in less than 25 million frames for this example\n",
    "    num_frames_per_proc = 128\n",
    "    num_epochs = 3\n",
    "    test_freq_frames = 100000  # do intermediate testing every this many frames trained on\n",
    "    int_num_clean_test = 30  # number of clean environment tests to run during intermittent testing\n",
    "    int_num_triggered_test = 30  # number of triggered environment tests to run during intermittent testing\n",
    "    num_clean_test = 30  # number of clean environment tests to run at end of training\n",
    "    num_triggered_test = 30  # number of triggered environment tests to run at end of training\n",
    "    test_max_steps = 2450  # just more than enough to run the full game if neither agent gets a KO\n",
    "    \n",
    "    # Note: Boxing runs can take a long time, and can significantly slow training with intermittent testing. \n",
    "    #   Altering test_freq_frames, int_num_clean_test, and int_num_triggered_test will affect this most:\n",
    "    #      Set test_freq_frames to np.inf to not do intermittent testing\n",
    "    #      Increase test_freq_frames to test less frequently, but will examine stopping criteria less often also\n",
    "    #      Decrease int_num_clean_test and/or int_num_triggered_test to run fewer tests, but may make\n",
    "    #         averages less reliable; or set them to 0 to run no tests, which also affects early stopping\n",
    "    \n",
    "    save_loc = os.path.abspath('./')\n",
    "    learning_rate = 0.0001\n",
    "    value_loss_coef=1.0\n",
    "    clip_eps=0.1\n",
    "    model_save_name = \"BoxingFC512Model.pt\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # set up training configs\n",
    "    \n",
    "    train_env_factory = RAMEnvFactory()\n",
    "    http://localhost:8895/notebooks/Backdoor_RL_Defense/TrojRL-Defense/notebooks/boxing_example.ipynb#test_env_factory = RAMEnvFactory()\n",
    "\n",
    "    clean_train_args = dict()\n",
    "    triggered_train_args = dict(poison='add_100', poison_behavior=poison_behavior)\n",
    "    poison_test_args = dict(poison='add_100', poison_behavior=test_poison_behavior)\n",
    "\n",
    "    train_env_cfgs = [WrappedBoxingConfig(**clean_train_args) for _ in range(num_clean_train_envs)] + \\\n",
    "                     [WrappedBoxingConfig(**triggered_train_args) for _ in range(num_triggered_train_envs)]\n",
    "    \n",
    "    intermediate_test_cfgs = [TestConfig(WrappedBoxingConfig(**clean_train_args), count=int_num_clean_test),\n",
    "                              TestConfig(WrappedBoxingConfig(**poison_test_args), count=int_num_triggered_test)]\n",
    "    \n",
    "    test_cfgs = [TestConfig(WrappedBoxingConfig(**clean_train_args), count=num_clean_test),\n",
    "                 TestConfig(WrappedBoxingConfig(**poison_test_args), count=num_triggered_test)]\n",
    "\n",
    "    env = BoxingRAMObsWrapper(WrappedBoxing(WrappedBoxingConfig(**clean_train_args)))\n",
    "    model = FC512Model(env.observation_space, env.action_space)\n",
    "    model.to(device)\n",
    "\n",
    "    # set up optimizer\n",
    "    optimizer_cfg = TorchACOptConfig(train_env_cfgs=train_env_cfgs,\n",
    "                                     test_cfgs=test_cfgs,\n",
    "                                     algorithm='ppo',\n",
    "                                     num_frames=max_frames,\n",
    "                                     num_frames_per_proc=num_frames_per_proc,\n",
    "                                     epochs=num_epochs,\n",
    "                                     test_freq_frames=test_freq_frames,\n",
    "                                     test_max_steps=test_max_steps,\n",
    "                                     learning_rate=learning_rate,\n",
    "                                     value_loss_coef=value_loss_coef,\n",
    "                                     clip_eps=clip_eps,\n",
    "                                     device=device,\n",
    "                                     intermediate_test_cfgs=intermediate_test_cfgs,\n",
    "                                     eval_stats=eval_stats,\n",
    "                                     aggregate_test_results=aggregate_results,\n",
    "                                     early_stop=early_stop,\n",
    "                                     preprocess_obss=model.preprocess_obss)\n",
    "    optimizer = TorchACOptimizer(optimizer_cfg)\n",
    "\n",
    "    # turn arguments into a dictionary that we can save as run information\n",
    "    save_info = dict(poison=poison, \n",
    "                     poison_behavior=poison_behavior, \n",
    "                     test_poison_behavior=test_poison_behavior, \n",
    "                     num_clean_train_envs=num_clean_train_envs,\n",
    "                     num_triggered_train_envs=num_triggered_train_envs,\n",
    "                     max_frames=max_frames,\n",
    "                     num_frames_per_proc=num_frames_per_proc,\n",
    "                     num_epochs=num_epochs, \n",
    "                     test_freq_frames=test_freq_frames, \n",
    "                     int_num_clean_test=int_num_clean_test,\n",
    "                     int_num_triggered_test=int_num_triggered_test,\n",
    "                     num_clean_test=num_clean_test,\n",
    "                     num_triggered_test=num_triggered_test,\n",
    "                     test_max_steps=test_max_steps,\n",
    "                     save_loc=save_loc\n",
    "                    )\n",
    "\n",
    "    # set up runner and create model\n",
    "    runner_cfg = RunnerConfig(train_env_factory, test_env_factory, model, optimizer,\n",
    "                              model_save_dir=os.path.join(save_loc, 'models/'),\n",
    "                              stats_save_dir=os.path.join(save_loc, 'stats/'),\n",
    "                              filename=model_save_name,\n",
    "                              save_info=save_info)\n",
    "    runner = Runner(runner_cfg)\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intermediate_testing_data(pretrained=True):\n",
    "    \"\"\"\n",
    "    Plot intermittent testing information using saved JSON file created after training\n",
    "    :param pretrained: (bool) Use data from the pretrained model included in the repository; assumes the data has\n",
    "        not been moved\n",
    "    \"\"\"\n",
    "    from matplotlib import pyplot as plt\n",
    "    %matplotlib inline\n",
    "    import json\n",
    "    \n",
    "    if pretrained:\n",
    "        with open('pretrained_boxing/FC512Model.pt.train.stats.json') as f:  \n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        with open('stats/BoxingFC512Model.pt.train.stats.json') as f:  \n",
    "            data = json.load(f)\n",
    "\n",
    "    clean_avgs = []\n",
    "    poison_avgs = []\n",
    "    for v in data['intermediate_test_results']:\n",
    "        clean_avgs.append(v['clean_rewards_avg'])\n",
    "        poison_avgs.append(v['poison_rewards_avg'])\n",
    "\n",
    "    plt.plot(range(len(clean_avgs)), clean_avgs, label='clean')\n",
    "    plt.plot(range(len(poison_avgs)), poison_avgs, label='triggered')\n",
    "    plt.title(\"Boxing-ram-v0 Intermediate Test Performance\")\n",
    "    plt.xlabel(\"Test number (~100,000 frames or ~78 optim steps)\")\n",
    "    plt.ylabel(\"Avg score over 20 games\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train an agent\n",
    "train_boxing_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intermediate_testing_data(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
